# Testing LLMs as a Juggling Tutor
This repository documents my exploration of how structured prompting can shape a large language model’s (LLM) behavior in an extended communicative task — specifically, a tutoring session on chess. The goal of this project is to compare the differences in tutoring style, responsiveness, and engagement between an unstructured (vanilla) LLM session and a structured LLM session guided by the **Mollick Tutoring Prompt**.
**Session_One_Vanilla.md** - Transcript and analysis of the first tutoring session with *no structured prompt*.  
**Session_Two_Structured.md** – Transcript and analysis of the tutoring session using the *Mollick Tutoring Prompt*.  
**Session_Three_Modified.md** – Optional third session testing a *customized tutoring prompt* based on my observations.  
